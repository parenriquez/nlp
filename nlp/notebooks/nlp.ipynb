{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Natural Language Processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. What is a Corpus?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **corpus** is a collection of authentic text or audio collected for a particular research project. It can be thought of as a **paragraph**, which is composed of **sentences** called **documents**, which then are composed of **vocabularies**, which are all the unique words found in all sentences.\n",
    "\n",
    "A corpus can be made up of everything from newspapers, novels, recipes, radio broadcasts to television shows, movies, and tweets. In natural language processing, a corpus is transformed into a **dataset**, which contains text and speech data that can be used to train AI and machine learning systems, and with labels for supervised learning.\n",
    "\n",
    "Features of a good corpus:\n",
    "\n",
    "* large quantities of specialized datasets are vital to training algorithms designed to perform sentiment analysis.\n",
    "* high-quality - due to the large volume of data required for a corpus, even minuscule errors in the training data can lead to large-scale errors in the machine learning system’s output.\n",
    "* clean from errors or duplicate data to create a more reliable corpus for NLP.\n",
    "* a high quality corpus is a balanced corpus - if one doesn’t streamline and structure the data collection process, it could unbalance the relevance of the dataset\n",
    "\n",
    "Challenges of creating a corpus:\n",
    "* deciding the type of data needed to solve the problem statement\n",
    "* availability of data\n",
    "* quality of the data\n",
    "* adequacy of data in terms of amount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**An example of a corpus**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This corpus is a collection of selected English tweets from 2013 to 2016 which are compiled as part of the **SemEval-2017** Task 4 Competition: *Sentiment Analysis on Twitter.*\n",
    "\n",
    "The collection of tweets are already labeled based on its sentiment: **negative**, **neutral**, and **positive** - which makes it a dataset suitable for supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, csv\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../datasets/2017_English_final/Subtask_A/'\n",
    "train_files = [\n",
    "    'twitter-2013train-A.txt',\n",
    "    'twitter-2013dev-A.txt',\n",
    "    'twitter-2013test-A.txt',\n",
    "    'twitter-2014sarcasm-A.txt', \n",
    "    'twitter-2014test-A.txt',\n",
    "    'twitter-2015train-A.txt',\n",
    "    'twitter-2015test-A.txt',\n",
    "    'twitter-2016train-A.txt',\n",
    "    'twitter-2016dev-A.txt',\n",
    "    'twitter-2016devtest-A.txt',\n",
    "    'twitter-2016test-A.txt',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of examples for training: 49675\n",
      "Distribution of classes:\n",
      "neutral     0.448032\n",
      "positive    0.395994\n",
      "negative    0.155974\n",
      "Name: label, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>640329403277438976</th>\n",
       "      <td>neutral</td>\n",
       "      <td>[ARIRANG] SIMPLY KPOP - Kim Hyung Jun - Cross ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640810454730833920</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@TyTomlinson just read a politico article abou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111344128507392000</th>\n",
       "      <td>neutral</td>\n",
       "      <td>I just typed in \"the Bazura Project\" into goog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641414049083691009</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Fast Lerner: Subpoenaed tech guy who worked on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637666734300905472</th>\n",
       "      <td>negative</td>\n",
       "      <td>Sony rewards app is like a lot of 19 y.o femal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       label  \\\n",
       "id                             \n",
       "640329403277438976   neutral   \n",
       "640810454730833920   neutral   \n",
       "111344128507392000   neutral   \n",
       "641414049083691009   neutral   \n",
       "637666734300905472  negative   \n",
       "\n",
       "                                                              message  \n",
       "id                                                                     \n",
       "640329403277438976  [ARIRANG] SIMPLY KPOP - Kim Hyung Jun - Cross ...  \n",
       "640810454730833920  @TyTomlinson just read a politico article abou...  \n",
       "111344128507392000  I just typed in \"the Bazura Project\" into goog...  \n",
       "641414049083691009  Fast Lerner: Subpoenaed tech guy who worked on...  \n",
       "637666734300905472  Sony rewards app is like a lot of 19 y.o femal...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_dataframe(file_path):\n",
    "    return pd.read_csv(\n",
    "        file_path, \n",
    "        sep='\\t',\n",
    "        quoting=csv.QUOTE_NONE,\n",
    "        usecols=[0,1,2],\n",
    "        names=['id', 'label', 'message'],\n",
    "        index_col=0,\n",
    "        dtype={'label': 'category'})\n",
    "\n",
    "train_dfs = []\n",
    "for f in train_files:\n",
    "    train_dfs.append(load_dataframe(os.path.join(data_dir, f)))\n",
    "tweets_train = pd.concat(train_dfs)\n",
    "# Dropping duplicates, as mentioned in its README there are 665 duplicate annotations across and within the files of Subtask_A\n",
    "tweets_train.drop_duplicates(inplace=True)\n",
    "# Dropping null records, either without label, or without message\n",
    "tweets_train.dropna(inplace=True)\n",
    "# Randomizing the arrangement of the records\n",
    "tweets_train = tweets_train.sample(frac=1.0, random_state=42)\n",
    "\n",
    "\n",
    "# Clean and prepare messages:\n",
    "def preprocess_messages(messages):\n",
    "    \n",
    "    messages = messages.str.decode('unicode_escape', errors='ignore')\n",
    "    messages = messages.str.strip('\"')  # remove left-most and right-most quotation mark\n",
    "    messages = messages.str.replace('\"\"', '\"', regex=False) # replacing double quotation to single quotation\n",
    "    \n",
    "    return messages\n",
    "\n",
    "tweets_train['message'] = preprocess_messages(tweets_train['message'])\n",
    "\n",
    "print('Total number of examples for training: {}\\nDistribution of classes:\\n{}'.format(\n",
    "    len(tweets_train),\n",
    "    tweets_train['label'].value_counts() / len(tweets_train),\n",
    "))\n",
    "\n",
    "tweets_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sony rewards app is like a lot of 19 y.o female singers and a non retro sale. 2nd one with no info'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_train.message.iloc[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative', 'neutral', 'positive']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mapping the labels 'negative', 'neutral' and 'positive' into 0, 1, 2\n",
    "tweets_train_y = tweets_train['label'].cat.codes\n",
    "labels = tweets_train.label.cat.categories.tolist()\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'negative': 0, 'neutral': 1, 'positive': 2}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_codes = {}\n",
    "for i, label in enumerate(labels):\n",
    "    labels_codes[label] = i\n",
    "\n",
    "labels_codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we cannot simply give these sentences to a machine learning model and ask it to tell us whether a review was positive or negative or neutral. We need to perform certain text preprocessing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text Processing** - from **Text** to **Vectors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I. Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "640329403277438976    arirang simply kpop kim hyung jun cross ha yeo...\n",
       "640810454730833920    read politico article donald trump running mat...\n",
       "111344128507392000    type bazura project google image image photo d...\n",
       "641414049083691009    fast lerner subpoena tech guy work hillary pri...\n",
       "637666734300905472    sony reward app like lot female singer non ret...\n",
       "264185448358875136    watch brooklyn nets new york knick tonight pos...\n",
       "636407569108586496                   guy open gate naruto save ass goat\n",
       "633549773337964545    triple h never ric flair bitch sunday no press...\n",
       "622833484571254784    joint leader amateur paul dunne win open champ...\n",
       "522787955216482304             glenn beck owner box redskin game sunday\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class TweetTokenizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    inherits the BaseEstimator and TransformerMixin (which contains the fit and transform functions) class from sklearn\n",
    "    used spacy for tokenization and lemmatization\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # initializing spacy pipeline\n",
    "        self.nlp = spacy.load('en_core_web_sm', disable = ['ner', 'parser', 'textcat'])\n",
    "        self.stops = self.nlp.Defaults.stop_words\n",
    "        # Removing negation words from the default stopwords set\n",
    "        # not, cannot, no, never, nothing, none, without, nor, neither, nobody, nowhere\n",
    "        # This is so we can keep the negative sentiment of a tweet brought by these words\n",
    "        negation_words = ['not','cannot','no', 'never', 'nothing','none','without','nor','neither','nobody','nowhere']\n",
    "        \n",
    "        for neg in negation_words:\n",
    "            self.stops.remove(neg)\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, messages):\n",
    "    \n",
    "        # Replace all whitespace characters by only one space\n",
    "        messages = messages.str.replace(r'\\s+', ' ',regex=True)\n",
    "        messages = messages.str.strip()\n",
    "        messages = messages.str.lower()\n",
    "\n",
    "        # returns a lemmatized version of a token if it is not a stop word and is an alphabet character\n",
    "        return messages.apply(lambda msg: \" \".join([token.lemma_ for token in self.nlp(msg) if token.lemma_.lower() not in self.stops and token.is_alpha]))\n",
    "\n",
    "# let's see some examples:\n",
    "tweets_train_tokenized = TweetTokenizer().fit_transform(tweets_train['message'])\n",
    "tweets_train_tokenized[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_train_tokenized.to_csv(\"csvs/tweets_train_tokens.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "640329403277438976    1\n",
       "640810454730833920    1\n",
       "111344128507392000    1\n",
       "641414049083691009    1\n",
       "637666734300905472    0\n",
       "                     ..\n",
       "264260341070954497    0\n",
       "641411364641206277    1\n",
       "636722845599469568    1\n",
       "264084248057765888    1\n",
       "276099025340612608    1\n",
       "Length: 49675, dtype: int8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_train_y.to_csv(\"csvs/tweets_train_y.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **End. Thank you!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "dcad393ebe0ddd96229d28636729608eefe2a539d1a2b16c2babaf1f7828873b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
